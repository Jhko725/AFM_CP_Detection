{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    \"see https://arxiv.org/pdf/1710.05941.pdf\"\n",
    "    return x / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def f(params, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0 objective [[1.11472535]]\n",
      "Iteration 1000 objective [[0.00049768]]\n",
      "Iteration 2000 objective [[0.0004579]]\n",
      "Iteration 3000 objective [[0.00041697]]\n",
      "Iteration 4000 objective [[0.00037408]]\n",
      "Iteration 5000 objective [[0.00033705]]\n",
      "Iteration 6000 objective [[0.00031016]]\n",
      "Iteration 7000 objective [[0.00029197]]\n",
      "Iteration 8000 objective [[0.00027585]]\n",
      "Iteration 9000 objective [[0.00024616]]\n",
      "f(0) = [[-0.00014613]]\n",
      "fp(0) = 0.0003518041251638071\n",
      "fp(6) = 0.999518061473252\n",
      "fpp(0) = 0.32633705037026633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 4.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is our initial guess of params:\n",
    "params = init_random_params(0.1, layer_sizes=[1, 8, 1])\n",
    "\n",
    "# Derivatives\n",
    "fp = elementwise_grad(f, 1)\n",
    "fpp = elementwise_grad(fp, 1)\n",
    "fppp = elementwise_grad(fpp, 1)\n",
    "\n",
    "eta = np.linspace(0, 6).reshape((-1, 1))\n",
    "\n",
    "# This is the function we seek to minimize\n",
    "def objective(params, step):\n",
    "    # These should all be zero at the solution\n",
    "    # f''' + 0.5 f'' f = 0\n",
    "    zeq = fppp(params, eta) + 0.5 * f(params, eta) * fpp(params, eta) \n",
    "    bc0 = f(params, 0.0)  # equal to zero at solution\n",
    "    bc1 = fp(params, 0.0)  # equal to zero at solution\n",
    "    bc2 = fp(params, 6.0) - 1.0 # this is the one at \"infinity\"\n",
    "    return np.mean(zeq**2) + bc0**2 + bc1**2 + bc2**2\n",
    "\n",
    "def callback(params, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective(params, step)))\n",
    "\n",
    "params = adam(grad(objective), params,\n",
    "              step_size=0.001, num_iters=10000, callback=callback) \n",
    "\n",
    "print('f(0) = {}'.format(f(params, 0.0)))\n",
    "print('fp(0) = {}'.format(fp(params, 0.0)))\n",
    "print('fp(6) = {}'.format(fp(params, 6.0)))\n",
    "print('fpp(0) = {}'.format(fpp(params, 0.0)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eta, f(params, eta))\n",
    "plt.xlabel('$\\eta$')\n",
    "plt.ylabel('$f(\\eta)$')\n",
    "plt.xlim([0, 6])\n",
    "plt.ylim([0, 4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special.factorial as fact\n",
    "k = 3\n",
    "df = [f] # list for storing derivatives of f\n",
    "for i in range(2*k):\n",
    "    df.append(elementwise_grad(df[i], 1))\n",
    "\n",
    "diffeq = (F/2)*np.sin(theta) + (A/2)*(k-m*w**2)\n",
    "bc = 0\n",
    "\n",
    "for i in range(k):\n",
    "    diffeq += np.power(A, 2*k+1)*df[2*k+1]/(2**(2*k+1)*fact(k)*fact(k+1))\n",
    "\n",
    "objective = np.mean(diffeq)**2\n",
    "for i in range(2*k):\n",
    "    objective += df[i](z[-1]) # need to flip z so that the largest z corresponds with z[-1] not z[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
